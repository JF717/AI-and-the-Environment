{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will build a simple feed forward artificial Neural network from scratch\n",
    "\n",
    "First step by step and then functionise it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first of all we will need some packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets create a small numpy array of input data, this is the same as our deforestation data\n",
    "Input = np.array([10,1,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  1,  4])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#take a look at it\n",
    "Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ok for dimensional reasons we want to transpose our Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input = Input.reshape((Input.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10],\n",
       "       [ 1],\n",
       "       [ 4]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the lecture we had a simple structure:\n",
    "\n",
    "3 Input : 4 Hidden Layer : 1 Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So lets generate an array that is the weights our hidden layer of size 4 \n",
    "# the weights need to be generated randomly and we will generate them between -4 and +4 for ease\n",
    "mean = 0\n",
    "stddev = 3\n",
    "Hidden = np.array([random.gauss(mean,stddev) for _ in range(12)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.20681229,  5.35992724,  1.51177548, -1.17093425,  4.54976766,\n",
       "       -2.22912093,  0.1594277 ,  1.37598663,  1.25677934, -1.30370169,\n",
       "       -0.52900196, -1.88578714])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We want to make sure that we have the correct dimensions here\n",
    "Hidden = Hidden.reshape(len(Input),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.20681229,  5.35992724,  1.51177548, -1.17093425],\n",
       "       [ 4.54976766, -2.22912093,  0.1594277 ,  1.37598663],\n",
       "       [ 1.25677934, -1.30370169, -0.52900196, -1.88578714]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets also generate some random biases\n",
    "BiasHidden = np.array([random.gauss(mean,stddev) for _ in range(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.20959402,  2.32626214,  5.0973965 ,  1.39541992])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BiasHidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ok so we have our input and our hidden layer now we want to work out the first layer\n",
    "#which means we need to do our first multiplication\n",
    "\n",
    "Layer1 = np.dot(Input.T,Hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 41.64500794,  46.15534474,  13.16117471, -17.87650441]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#And here are our outputs for layer 1 after multiplication\n",
    "Layer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remember to add the biases\n",
    "Layer1 = Layer1+BiasHidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 40.43541392,  48.48160687,  18.25857121, -16.48108449]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Layer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we have to apply our activation functions, we covered a few activation functions in the lecture and specifically for the hidden layer\n",
    "#we looked at ReLu and tanh so we will define them here as functions\n",
    "\n",
    "#ReLU is simple as it just turns any negative values to 0\n",
    "def ReLU(x):\n",
    "    return np.maximum(x,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[40.43541392, 48.48160687, 18.25857121,  0.        ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ReLU(Layer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tanh is a bit more complicated taking the form e^x - e^-x / e^x + e^-x\n",
    "def tanh(x):\n",
    "    ex = np.exp(x)\n",
    "    enx = np.exp(-x)\n",
    "    return (ex - enx) / (ex + enx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1., -1.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanh(Layer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1., -1.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tanh is actually already inside numpy\n",
    "np.tanh(Layer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets use tanh as our activation function \n",
    "Layer1out = tanh(Layer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1., -1.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Layer1out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next we will need the weights of our output layer we are trying to collapse 4 into 1 so we need 4 weights\n",
    "OutWeights = np.array([random.gauss(mean,stddev) for _ in range(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.67928807, -3.13259297,  6.1587629 ,  0.79561749])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OutWeights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Out = np.dot(Layer1out,OutWeights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.90984051])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need a bias for the final one\n",
    "BiasOut = np.array([random.gauss(mean,stddev) for _ in range(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.09625819])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BiasOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Out += BiasOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we need our final activation function the sigmoid function\n",
    "def Sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "OutPrediction = Sigmoid(Out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99702218])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OutPrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ok we have done a complete forward pass through our neural network and the model is predicting this area will be deforested with a probability\n",
    "#as this area was deforested it should be 1 so lets calculate the error, we will make a function that does this\n",
    "def LogLoss(x):\n",
    "    return -np.log(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "Error = LogLoss(OutPrediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00298226])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we need to begin our back propagation with this error the first step will need the derivative of the sigmoid function\n",
    "# the dsigmoid = sigmoid * 1 - Sigmoid\n",
    "def SigmoidDerivative(x):\n",
    "    return (1 / (1 + np.exp(-x))) * (1 - (1 + np.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we also need our log loss derivative which is just -1/-logloss\n",
    "def DLogLoss(x):\n",
    "    return 1/(-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#our error composite then becomes DLogLoss * Sigmoid Derivative\n",
    "Errorcomp = DLogLoss(Error) * SigmoidDerivative(OutPrediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([90.37685675])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Errorcomp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we back propagate this through the weights\n",
    "grads1 = np.dot(Errorcomp, Layer1out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 90.37685675,  90.37685675,  90.37685675, -90.37685675])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now to make the change in bias which is the sum of the changes in weights\n",
    "bias1 = sum(grads1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180.7537135066233"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next we need to backcast the error into the final layer of weights\n",
    "#for this we will need the differential of the tanh activation function that we used\n",
    "def DTanh(x):\n",
    "    return 1 - np.tanh(x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "DLayer1 = DTanh(Layer1out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.41997434, 0.41997434, 0.41997434, 0.41997434]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DLayer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we multiple this by our change in the last layer or grads 1\n",
    "Dstep1 = DLayer1 * (Errorcomp * OutWeights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 139.65091411, -118.90057616,  233.76176393,   30.19842648]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dstep1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have this step which is the error and the differential of layer 1 together we now need to multiply this by the input to get our weights\n",
    "grads2 = np.dot(Input,Dstep1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1396.50914111, -1189.00576161,  2337.61763932,   301.98426476],\n",
       "       [  139.65091411,  -118.90057616,   233.76176393,    30.19842648],\n",
       "       [  558.60365644,  -475.60230464,   935.04705573,   120.7937059 ]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias2 = sum(sum(grads2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-125.10991423501088"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we have our changes in weights and changes in biases we just need to add these to our weights and biases\n",
    "Hidden += grads2\n",
    "OutWeights += grads1\n",
    "BiasHidden += bias2\n",
    "BiasOut += bias1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that is that, we have updated all the weights and biases in one full pass if we run it through again it would hopefully perform better\n",
    "\n",
    "Now we should turn it into a nice function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets start with a function that creates the network\n",
    "def InitialiseNetwork(Inputsize,HiddenSize,OutSize):\n",
    "    mean = 0\n",
    "    stddev = 3\n",
    "    Hidden = np.array([random.gauss(mean,stddev) for _ in range(Inputsize * HiddenSize)])\n",
    "    Hidden = Hidden.reshape(Inputsize,HiddenSize)\n",
    "    BiasHidden = np.array([random.gauss(mean,stddev) for _ in range(HiddenSize)])\n",
    "    OutWeights = np.array([random.gauss(mean,stddev) for _ in range(HiddenSize * OutSize)])\n",
    "    OutWeights = OutWeights.reshape(OutSize,HiddenSize)\n",
    "    BiasOut = np.array([random.gauss(mean,stddev) for _ in range(OutSize)])\n",
    "    return dict(Hidden = Hidden, BiasHidden = BiasHidden,OutWeights = OutWeights,BiasOut = BiasOut )\n",
    "    \n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "Net1 = InitialiseNetwork(3,4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hidden': array([[-1.68627158, -0.36213892,  0.58467549,  5.25495914],\n",
       "        [ 2.30930193, -4.125814  , -0.81786148,  6.35209179],\n",
       "        [-0.61522387, -1.08612155,  1.05474095, -0.67232774]]),\n",
       " 'BiasHidden': array([ 1.78557522,  2.12804168, -2.39568901,  4.68914505]),\n",
       " 'OutWeights': array([[ 0.5392547 ,  1.28473714, -0.56769683,  5.80142289]]),\n",
       " 'BiasOut': array([0.17311207])}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Net1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ForwardPass(Input,Net):\n",
    "    Forward1 = tanh((np.dot(Input.T,Net['Hidden'])) + Net['BiasHidden'])\n",
    "    Forward2 = Sigmoid(np.dot(Forward1,Net['OutWeights'].T)+Net['BiasOut'])\n",
    "    return dict(Forward1 = Forward1, Forward2 = Forward2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "FW1 = ForwardPass(Input,Net1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Forward1': array([[-1.        , -1.        ,  0.99999776,  1.        ]]),\n",
       " 'Forward2': array([[0.97295531]])}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BackwardsPass(Input,Forward,Net):\n",
    "    Error = LogLoss(Forward['Forward2'])\n",
    "    Errorcomp = DLogLoss(Forward['Forward2']) * SigmoidDerivative(Forward['Forward2'])\n",
    "    grads1 = Errorcomp * Forward['Forward1']\n",
    "    bias1 = sum(sum(grads1))\n",
    "    grads2 = np.dot(Input,(Errorcomp * Net['OutWeights'] * DTanh(Forward['Forward1'])))\n",
    "    bias2 = sum(sum(grads2))\n",
    "    Net['Hidden'] += grads2\n",
    "    Net['BiasHidden'] += bias2\n",
    "    Net['OutWeights'] += grads1\n",
    "    Net['BiasOut'] += bias1\n",
    "    return Net, Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "Net2,Err = BackwardsPass(Input,FW1,Net1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hidden': array([[-1.04780718,  1.15895851, -0.08746598, 12.12370222],\n",
       "        [ 2.37314837, -3.97370426, -0.88507562,  7.0389661 ],\n",
       "        [-0.35983811, -0.47768258,  0.78588437,  2.0751695 ]]),\n",
       " 'BiasHidden': array([14.31982039, 14.66228685, 10.13855615, 17.22339022]),\n",
       " 'OutWeights': array([[ 0.25733853,  1.00282097, -0.28578128,  6.08333906]]),\n",
       " 'BiasOut': array([0.17311144])}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Net2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0004194]])"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go if we run this over and over on the same data it will improve\n",
    "\n",
    "Try than now using a for loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next thing to do is to make the neural network a class. All the functions are there for you all you need to do is make a class that works with any size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final task for this practical we will build a neural network to classify some real world data. You should have two data sets called ForestClass either training or testing. These are real world datasets extracted from satelite images. The class variable is the type of forest that the data is from and the other data are all bandwidth measurements from band 1:9 of satelite images. Build a neural network that will predict the type of forest. For an easy version just try and classify between the two most common types or if you want more of a challenge you can try and classify between all of them, for which you will need the softmax function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#softmax is very much like the sigmoid function but for multiple classes\n",
    "def softmax(x):\n",
    "   #the softmax is defined as e^x-max(x) / sum(e^x-max(x))\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    #it will return an array that is the softmax of an array the highest value in the softmax is your predicted class\n",
    "    return e_x / e_x.sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the softmax derivative is incredibly complicated as it uses a masking jacobian matrix but you will also need it\n",
    "def softmax_dif(s):\n",
    "    #if this isn't working try and transpose your softmax output\n",
    "    jacobian_m = np.diag(s)\n",
    "\n",
    "    for i in range(len(jacobian_m)):\n",
    "        for j in range(len(jacobian_m)):\n",
    "            if i == j:\n",
    "                jacobian_m[i][j] = s[i] * (1-s[i])\n",
    "            else: \n",
    "                jacobian_m[i][j] = -s[i]*s[j]\n",
    "    return jacobian_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good Luck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the easiest thing is to use this function to go straight to the errorcomp \n",
    "def delta_cross_entropy(X,y):\n",
    "    \"\"\"\n",
    "    X is the output from the softmax layer (num_examples x num_classes)\n",
    "    y is labels array so e.g [0,0,1,0]\n",
    "    \tNote that y is not one-hot encoded vector. \n",
    "    \tIt can be computed as y.argmax(axis=1) from one-hot encoded vectors of labels if required.\n",
    "    \"\"\"\n",
    "    m = y.shape[0]\n",
    "    grad = softmax(X)\n",
    "    grad[range(m),y] -= 1\n",
    "    grad = grad/m\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
